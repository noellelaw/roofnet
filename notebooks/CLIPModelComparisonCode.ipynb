{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsODuMdLqXLf",
        "outputId": "0bf3b95f-615c-472f-bfc9-b0b262bc448e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJRNEUNzMPcW"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "!pip install torch torchvision transformers open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Material Descriptions and CSV Loading\n",
        "\n",
        "model_name = 'ViT-L-14' # ['RN50', 'ViT-B-32', 'ViT-L-14']\n",
        "\n",
        "# Load test CSV\n",
        "csv_path = \"test/roof_dataset_clip_prompts_test.csv\"  # Adjust path as needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Class prompts\n",
        "material_descriptions = {\n",
        "    \"Thatch\": \"thatch roof\",\n",
        "    \"GreenVegetative\": \"roof with vegetation on it\",\n",
        "    \"StoneSlates\": \"dark stone slate roof\",\n",
        "    \"ClayTiles\": \"clay tile / ceramic tile roof\",\n",
        "    \"AsphaltTiles\": \"asphalt shingle pitched roof\",\n",
        "    \"ConcreteTiles\": \"tiled concrete / tiled cement roof\",\n",
        "    \"WoodTiles\": \"wood shingle roof\",\n",
        "    \"MetalSheetMaterials\": \"corrugated or tiled metal roof (silver / dark / painted)\",\n",
        "    \"PolycarbonateSheetMaterials\": \"polycarbonate roof\",\n",
        "    \"GlassSheetMaterials\": \"glass roof (clear or mirrored)\",\n",
        "    \"AmorphousConcrete\": \"flat concrete / cement roof\",\n",
        "    \"AmorphousAsphalt\": \"asphalt-coated roof (bitumen layer or rolled roofing)\",\n",
        "    \"AmorphousMembrane\": \"membrane roof (bright EPDM / TPO)\",\n",
        "    \"AmorphousFabric\": \"tensile fabric roof (PVC / PTFE / canvas)\",\n",
        "    \"Unknown\": \"unknown material, image may be too low resolution or obstructed\"\n",
        "}\n",
        "classes = list(material_descriptions.keys())\n",
        "prompts = list(material_descriptions.values())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ERbU1D6GIfXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import open_clip\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import hf_hub_download\n"
      ],
      "metadata": {
        "id": "PiQBhr92p9Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test RoofNet Fine-tuned RemoteCLIP Model\n",
        "model, preprocess_train, preprocess = open_clip.create_model_and_transforms(model_name)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "ckpt = torch.load('models/roofnetxclip_model_subset.pth')\n",
        "message = model.load_state_dict(ckpt)\n",
        "print(message)\n",
        "\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "classes = list(material_descriptions.keys())\n",
        "prompts = list(material_descriptions.values())\n",
        "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "# Encode prompts\n",
        "with torch.no_grad():\n",
        "    text_tokens = tokenizer(prompts).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Evaluate\n",
        "correct = 0\n",
        "total = 0\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    img_path = os.path.join(\"test\", row[\"image\"])  # <-- Replace with your actual path\n",
        "    true_class = row[\"image\"].split(\"/\")[0]\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        sims = image_features @ text_features.T\n",
        "        pred_class = classes[sims.argmax().item()]\n",
        "\n",
        "    true_labels.append(class_to_idx[true_class])\n",
        "    pred_labels.append(class_to_idx[pred_class])\n",
        "\n",
        "# === Compute Accuracy ===\n",
        "correct = sum([t == p for t, p in zip(true_labels, pred_labels)])\n",
        "total = len(true_labels)\n",
        "print(f\"\\n✅ CLIP Top-1 Accuracy: {correct / total * 100:.2f}% ({correct}/{total})\")\n",
        "\n",
        "# === Generate and Plot Confusion Matrix ===\n",
        "cm = confusion_matrix(true_labels, pred_labels, labels=list(range(len(classes))))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(xticks_rotation=90, ax=ax, cmap='viridis')\n",
        "plt.title(\"CLIP Material Classification Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vK69DSkVDRO9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test RoofNet Fine-tuned RemoteCLIP Model with Class-Imbalanced Training\n",
        "model, preprocess_train, preprocess = open_clip.create_model_and_transforms(model_name)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "ckpt = torch.load('models/roofnetxclip_model_subset_balanced.pth')\n",
        "message = model.load_state_dict(ckpt)\n",
        "print(message)\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "classes = list(material_descriptions.keys())\n",
        "prompts = list(material_descriptions.values())\n",
        "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "# Encode prompts\n",
        "with torch.no_grad():\n",
        "    text_tokens = tokenizer(prompts).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Evaluate\n",
        "correct = 0\n",
        "total = 0\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    img_path = os.path.join(\"test\", row[\"image\"])  # <-- Replace with your actual path\n",
        "    true_class = row[\"image\"].split(\"/\")[0]\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        sims = image_features @ text_features.T\n",
        "        pred_class = classes[sims.argmax().item()]\n",
        "\n",
        "    true_labels.append(class_to_idx[true_class])\n",
        "    pred_labels.append(class_to_idx[pred_class])\n",
        "\n",
        "# === Compute Accuracy ===\n",
        "correct = sum([t == p for t, p in zip(true_labels, pred_labels)])\n",
        "total = len(true_labels)\n",
        "print(f\"\\n✅ CLIP Top-1 Accuracy: {correct / total * 100:.2f}% ({correct}/{total})\")\n",
        "\n",
        "# === Generate and Plot Confusion Matrix ===\n",
        "cm = confusion_matrix(true_labels, pred_labels, labels=list(range(len(classes))))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(xticks_rotation=90, ax=ax, cmap='viridis')\n",
        "plt.title(\"CLIP Material Classification Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BsB4qvEYSHgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test CLIP OOB ViT-14/L\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "classes = list(material_descriptions.keys())\n",
        "prompts = list(material_descriptions.values())\n",
        "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "# Encode prompts\n",
        "with torch.no_grad():\n",
        "    text_tokens = tokenizer(prompts).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Evaluate\n",
        "correct = 0\n",
        "total = 0\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    img_path = os.path.join(\"test\", row[\"image\"])  # <-- Replace with your actual path\n",
        "    true_class = row[\"image\"].split(\"/\")[0]\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        sims = image_features @ text_features.T\n",
        "        pred_class = classes[sims.argmax().item()]\n",
        "\n",
        "    true_labels.append(class_to_idx[true_class])\n",
        "    pred_labels.append(class_to_idx[pred_class])\n",
        "\n",
        "# === Compute Accuracy ===\n",
        "correct = sum([t == p for t, p in zip(true_labels, pred_labels)])\n",
        "total = len(true_labels)\n",
        "print(f\"\\n✅ CLIP Top-1 Accuracy: {correct / total * 100:.2f}% ({correct}/{total})\")\n",
        "\n",
        "# === Generate and Plot Confusion Matrix ===\n",
        "cm = confusion_matrix(true_labels, pred_labels, labels=list(range(len(classes))))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(xticks_rotation=90, ax=ax, cmap='viridis')\n",
        "plt.title(\"CLIP Material Classification Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RNWk77MNVkXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test OOB RemoteCLIP Model\n",
        "\n",
        "\n",
        "for model_name in ['ViT-L-14']:\n",
        "    checkpoint_path = hf_hub_download(\"chendelong/RemoteCLIP\", f\"RemoteCLIP-{model_name}.pt\", cache_dir='checkpoints')\n",
        "    print(f'{model_name} is downloaded to {checkpoint_path}.')\n",
        "model_name = 'ViT-L-14'\n",
        "model, preprocess_train, preprocess = open_clip.create_model_and_transforms(model_name)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "path_to_your_checkpoints = 'checkpoints/models--chendelong--RemoteCLIP/snapshots/bf1d8a3ccf2ddbf7c875705e46373bfe542bce38'\n",
        "\n",
        "ckpt = torch.load(f\"{path_to_your_checkpoints}/RemoteCLIP-{model_name}.pt\", map_location=\"cpu\")\n",
        "message = model.load_state_dict(ckpt)\n",
        "print(message)\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "classes = list(material_descriptions.keys())\n",
        "prompts = list(material_descriptions.values())\n",
        "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "# Encode prompts\n",
        "with torch.no_grad():\n",
        "    text_tokens = tokenizer(prompts).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Evaluate\n",
        "correct = 0\n",
        "total = 0\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    img_path = os.path.join(\"test\", row[\"image\"])  # <-- Replace with your actual path\n",
        "    true_class = row[\"image\"].split(\"/\")[0]\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        sims = image_features @ text_features.T\n",
        "        pred_class = classes[sims.argmax().item()]\n",
        "\n",
        "    true_labels.append(class_to_idx[true_class])\n",
        "    pred_labels.append(class_to_idx[pred_class])\n",
        "\n",
        "# === Compute Accuracy ===\n",
        "correct = sum([t == p for t, p in zip(true_labels, pred_labels)])\n",
        "total = len(true_labels)\n",
        "print(f\"\\n✅ CLIP Top-1 Accuracy: {correct / total * 100:.2f}% ({correct}/{total})\")\n",
        "\n",
        "# === Generate and Plot Confusion Matrix ===\n",
        "cm = confusion_matrix(true_labels, pred_labels, labels=list(range(len(classes))))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(xticks_rotation=90, ax=ax, cmap='viridis')\n",
        "plt.title(\"CLIP Material Classification Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PcbBQCTmV7Eg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}