{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJRNEUNzMPcW"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "!pip install torch torchvision transformers open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define RoofNet Path\n",
        "import os\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# UPDATE BELOW\n",
        "ROOFNET_DIR = \"\"\n",
        "\n",
        "\n",
        "# === Load Dataset & Compute Class Weights ===\n",
        "ROOFNET_SUBSET_DIR  = \"\"\n",
        "CSV_PATH = \"\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "class_names = sorted(df['image'].apply(lambda x: x.split('/')[0]).unique())\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "df['label'] = df['image'].apply(lambda x: class_to_idx[x.split('/')[0]])"
      ],
      "metadata": {
        "id": "xesZC8Yk6ZiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load packages and RemoteCLIP download model weights\n",
        "# Models from, Code adapted from https://github.com/ChenDelong1999/RemoteCLIP?tab=readme-ov-file\n",
        "from huggingface_hub import hf_hub_download\n",
        "import torch, open_clip\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "for model_name in ['ViT-L-14']:\n",
        "    checkpoint_path = hf_hub_download(\"chendelong/RemoteCLIP\", f\"RemoteCLIP-{model_name}.pt\", cache_dir='checkpoints')\n",
        "    print(f'{model_name} is downloaded to {checkpoint_path}.')"
      ],
      "metadata": {
        "id": "K1BYnYxNTG-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load in RemoteClip Model\n",
        "model_name = 'ViT-L-14'\n",
        "model, preprocess_train, preprocess = open_clip.create_model_and_transforms(model_name)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "path_to_your_checkpoints = 'checkpoints/models--chendelong--RemoteCLIP/snapshots/bf1d8a3ccf2ddbf7c875705e46373bfe542bce38'\n",
        "\n",
        "ckpt = torch.load(f\"{path_to_your_checkpoints}/RemoteCLIP-{model_name}.pt\", map_location=\"cpu\")\n",
        "message = model.load_state_dict(ckpt)\n",
        "print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0E4UyiiS5k6",
        "outputId": "a87ff41e-b059-46ff-b036-a1dbb9f47afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Finetune RemoteCLIP for 5 epochs using class rebalancing\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "import open_clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# === Dataset Definition ===\n",
        "class RoofDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.classes = self.data['image'].apply(lambda x: x.split('/')[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        return image, row['prompt'], row['image'].split('/')[0]  # returns class name\n",
        "\n",
        "# === Compute class weights\n",
        "class_counts = df['label'].value_counts().sort_index()\n",
        "weights = 1.0 / class_counts\n",
        "sample_weights = df['label'].map(weights).values\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_dataset = RoofDataset(CSV_PATH, ROOFNET_SUBSET_DIR, transform=preprocess_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
        "\n",
        "# === Training Setup ===\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "temperature = 0.07\n",
        "save_path = os.path.join(img_dir, \"best_clip_model_balanced.pth\")\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, texts, labels_str in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        tokenized_texts = tokenizer(texts).to(device)\n",
        "        labels = torch.tensor([class_to_idx[l] for l in labels_str], device=device)\n",
        "\n",
        "        image_features = model.encode_image(images)\n",
        "        text_features = model.encode_text(tokenized_texts)\n",
        "        normalized_image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        normalized_text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logits = (normalized_image_features @ normalized_text_features.T) / temperature\n",
        "        loss = (loss_fn(logits, torch.arange(len(images)).to(device)) +\n",
        "                loss_fn(logits.T, torch.arange(len(images)).to(device))) / 2\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} Avg Loss: {avg_loss:.4f}\")\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"New best model saved at epoch {epoch} with loss {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "MjZeWeQLa5JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Material Descriptions\n",
        "material_descriptions = {\n",
        "    \"Thatch\": \"thatch roof\",\n",
        "    \"GreenVegetative\": \"roof with vegetation on it\",\n",
        "    \"StoneSlates\": \"dark stone slate roof\",\n",
        "    \"ClayTiles\": \"clay / ceramic tile roof \",\n",
        "    \"AsphaltTiles\": \"asphalt shingle pitched roof\",\n",
        "    \"ConcreteTiles\": \"concrete / cement tile roof\",\n",
        "    \"WoodTiles\": \"wood shingle roof\",\n",
        "    \"MetalSheetMaterials\": \"corrugated or tiled metal roof (silver / dark / painted)\",\n",
        "    \"PolycarbonateSheetMaterials\": \"polycarbonate roof\",\n",
        "    \"GlassSheetMaterials\": \"glass roof (clear or mirrored)\",\n",
        "    \"AmorphousConcrete\": \"flat concrete roof\",\n",
        "    \"AmorphousAsphalt\": \"asphalt-coated roof (bitumen layer or rolled roofing)\",\n",
        "    \"AmorphousMembrane\": \"membrane roof (bright EPDM/TPO)\",\n",
        "    \"AmorphousFabric\": \"tensile fabric roof (PVC / PTFE / canvas)\",\n",
        "    \"Unknown\": \"unknown material, image may be too low resolution or obstructed\"\n",
        "}"
      ],
      "metadata": {
        "id": "-fh8VFvPbuEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FineTuned Image Classification\n",
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "\n",
        "# === CONFIG ===\n",
        "model_weights_path = os.path.join(DATASET_DIR, \"\") # <--Update with your model\n",
        "output_base_dir = \"\" # <--Update with your output directiory\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Your list of material classes (labels)\n",
        "material_classes = [\n",
        "    \"Thatch\", \"StoneSlates\", \"ClayTiles\", \"AsphaltTiles\",\n",
        "    \"ConcreteTiles\", \"WoodTiles\", \"MetalSheetMaterials\", \"PolycarbonateSheetMaterials\",\n",
        "    \"GlassSheetMaterials\", \"AmorphousConcrete\", \"AmorphousAsphalt\",\n",
        "    \"AmorphousMembrane\", \"AmorphousFabric\", \"Unknown\", \"GreenVegetative\"\n",
        "]\n",
        "\n",
        "model, _, _ = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                         std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
        "\n",
        "# Load your fine-tuned weights\n",
        "model.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Ensure output folders exist\n",
        "for material in material_classes:\n",
        "    os.makedirs(os.path.join(output_base_dir, material), exist_ok=True)\n",
        "\n",
        "# === Functions ===\n",
        "\n",
        "def build_prompts(city_name):\n",
        "    prompts = [f\"{material} in {city_name}\" for material in material_classes]\n",
        "    return prompts\n",
        "\n",
        "def extract_city_name_from_filename(filename):\n",
        "    base = os.path.splitext(os.path.basename(filename))[0]\n",
        "    base = Path(filename).stem\n",
        "    if '-' in base:\n",
        "        city_part = base.split('-')[0]\n",
        "        city_name = city_part.replace('_', ' ').title()\n",
        "        return city_name\n",
        "    elif 'height' in base:\n",
        "        city_part = base.split('_height')[0]\n",
        "        city_name = city_part.replace('_', ' ').title()\n",
        "        return city_name\n",
        "    elif 'imsat' in base:\n",
        "        city_part = base.split('_imsat')[0]\n",
        "        city_name = city_part.replace('_', ' ').title()\n",
        "    return city_name\n",
        "\n",
        "def already_classified(img_name):\n",
        "    \"\"\"Check if image already exists in any material folder.\"\"\"\n",
        "    for material in material_classes:\n",
        "        target_path = os.path.join(output_base_dir, material, img_name)\n",
        "        if os.path.exists(target_path):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def predict_and_move(image_path, city_name):\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Check area\n",
        "    width, height = image.size\n",
        "    area = width * height\n",
        "\n",
        "    if area <= 1000:\n",
        "        print(f\"{os.path.basename(image_path)} too small ({area} px), skipping.\")\n",
        "        return\n",
        "\n",
        "    # Preprocess for CLIP\n",
        "    image = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Build prompts\n",
        "    prompts = build_prompts(city_name)\n",
        "    tokenized_prompts = tokenizer(prompts).to(device)\n",
        "\n",
        "    # Encode\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(tokenized_prompts)\n",
        "\n",
        "    # Normalize\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Similarity\n",
        "    similarities = (100.0 * image_features @ text_features.T).squeeze(0)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    predicted_material = material_classes[best_idx]\n",
        "\n",
        "    # Move to correct folder\n",
        "    target_dir = os.path.join(output_base_dir, predicted_material)\n",
        "    try:\n",
        "      shutil.move(image_path, target_dir)\n",
        "\n",
        "      print(f\"{os.path.basename(image_path)} classified as {predicted_material} and moved.\")\n",
        "    except:\n",
        "      print(f\"{os.path.basename(image_path)} already present.\")\n",
        "\n",
        "# === Main Loop ===\n",
        "for material in material_classes:\n",
        "  input_images_dir = os.path.join(XBD_DATASET_DIR_TIER3)\n",
        "\n",
        "  for img_file in os.listdir(input_images_dir):\n",
        "      if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "          full_path = os.path.join(input_images_dir, img_file)\n",
        "          city_name = extract_city_name_from_filename(img_file)\n",
        "          predict_and_move(full_path, city_name)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-JRnZf6Kb5BM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}